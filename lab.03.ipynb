{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Лабораторная работа 3. Линейные методы.\n",
    "\n",
    "Результат лабораторной работы − отчет. Мы предпочитаем принимать отчеты в формате ноутбуков IPython (ipynb-файл). Постарайтесь сделать ваш отчет интересным рассказом, последовательно отвечающим на вопросы из заданий. Помимо ответов на вопросы, в отчете также должен быть код, однако чем меньше кода, тем лучше всем: нам − меньше проверять, вам — проще найти ошибку или дополнить эксперимент. При проверке оценивается четкость ответов на вопросы, аккуратность отчета и кода.\n",
    "\n",
    "Мы уверены, что выполнение лабораторных работ занимает значительное время, поэтому не рекомендуем оставлять их на последний вечер перед сдачей.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "* Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи)\n",
    "* Максимально допустимая оценка за работу — 15 баллов\n",
    "* Сдавать задание после указанного срока сдачи нельзя\n",
    "* «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов и понижают карму (подробнее о плагиате см. на странице курса)\n",
    "* Если вы нашли решение какого-то из заданий в открытом источнике, необходимо прислать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник)\n",
    "* Не оцениваются задания с удалёнными формулировкам\n",
    "* Не оценивается лабораторная работа целиком, если она была выложена в открытый источник\n",
    "\n",
    "Обратите внимание, что мы не ставим оценку за просто написанный код, корректная работоспособность которого не подтверждена экспериментами.\n",
    "\n",
    "### Правила сдачи\n",
    "Выполненную работу следует отправить в систему Anytask. Более подробно о системе можно почитать на странице курса. Название отправляемого файла должно иметь следующий формат: Surname_Name_Group_NN.ipynb, где NN — номер лабораторной работы. Например, Kozlova_Anna_CS_03.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='log_reg'></a>\n",
    "## Логистическая регрессия\n",
    "\n",
    "В этом пункте мы будем рассматривать бинарную классификацию, где метки классов лежат во множестве $\\{-1, 1\\}$. \n",
    "\n",
    "Задачу обучения регуляризованной логистической регрессии можно записать следующим образом:\n",
    "\n",
    "$$ \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + \\exp(-\\langle w, x_i \\rangle y_i)) + \\dfrac{C}{2}\\lVert w \\rVert^2  \\to \\min_w$$\n",
    "\n",
    "Обучение в данном случае сводится к нахождению параметров модели $w$, которое производится с помощью метода градиентного спуска (Gradient Descent, GD). \n",
    "\n",
    "Градиентный шаг будет заключаться в обновлении вектора весов по следующей формуле:\n",
    "\n",
    "$$w := w + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-\\langle w, x_i \\rangle y_i)}\\Big) - \\eta Cw$$\n",
    "\n",
    "где $\\eta > 0$ — размер шага (learning rate).\n",
    "\n",
    "В общем случае метод градиентного спуска имеет следующие недостатки:\n",
    "- попадание в локальные минимумы\n",
    "- неочевидность критерия останова\n",
    "- выбор размера шага\n",
    "- начальная инициализация весов\n",
    "\n",
    "В этой части лабораторной работы мы предложим вам реализовать метод градиентного спуска, а также рассмотрим некоторые его модификации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте выборку из 1000 точек с 20 признаками на которой будете проводить эксперименты. Мы рекомендуем воспользоваться функцией [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) из пакета sklearn. Обратите внимание, что метки классов для данной задачи должны быть из множества {-1, 1} (по умолчанию make_classification возвращает метки из множества {0, 1})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.116442</td>\n",
       "      <td>0.015345</td>\n",
       "      <td>0.125402</td>\n",
       "      <td>-0.098801</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>-0.103587</td>\n",
       "      <td>-0.012094</td>\n",
       "      <td>-0.079076</td>\n",
       "      <td>0.057976</td>\n",
       "      <td>-0.004344</td>\n",
       "      <td>-0.052137</td>\n",
       "      <td>-0.061970</td>\n",
       "      <td>0.028560</td>\n",
       "      <td>0.077146</td>\n",
       "      <td>0.051469</td>\n",
       "      <td>-0.029513</td>\n",
       "      <td>-0.089124</td>\n",
       "      <td>-0.133571</td>\n",
       "      <td>-0.024268</td>\n",
       "      <td>0.013084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288796</td>\n",
       "      <td>0.297414</td>\n",
       "      <td>0.288240</td>\n",
       "      <td>0.278640</td>\n",
       "      <td>0.335903</td>\n",
       "      <td>0.318658</td>\n",
       "      <td>0.269757</td>\n",
       "      <td>0.307727</td>\n",
       "      <td>0.270874</td>\n",
       "      <td>0.399235</td>\n",
       "      <td>0.268205</td>\n",
       "      <td>0.313365</td>\n",
       "      <td>0.312077</td>\n",
       "      <td>0.321633</td>\n",
       "      <td>0.302334</td>\n",
       "      <td>0.273468</td>\n",
       "      <td>0.319073</td>\n",
       "      <td>0.301684</td>\n",
       "      <td>0.285178</td>\n",
       "      <td>0.281340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.303993</td>\n",
       "      <td>-0.191731</td>\n",
       "      <td>-0.055399</td>\n",
       "      <td>-0.286264</td>\n",
       "      <td>-0.205822</td>\n",
       "      <td>-0.309697</td>\n",
       "      <td>-0.201008</td>\n",
       "      <td>-0.275632</td>\n",
       "      <td>-0.114466</td>\n",
       "      <td>-0.302975</td>\n",
       "      <td>-0.217847</td>\n",
       "      <td>-0.278649</td>\n",
       "      <td>-0.169166</td>\n",
       "      <td>-0.139485</td>\n",
       "      <td>-0.147503</td>\n",
       "      <td>-0.195425</td>\n",
       "      <td>-0.297408</td>\n",
       "      <td>-0.331173</td>\n",
       "      <td>-0.214304</td>\n",
       "      <td>-0.179786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.102595</td>\n",
       "      <td>0.030105</td>\n",
       "      <td>0.126738</td>\n",
       "      <td>-0.108769</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>-0.116210</td>\n",
       "      <td>-0.030115</td>\n",
       "      <td>-0.077754</td>\n",
       "      <td>0.054520</td>\n",
       "      <td>-0.030569</td>\n",
       "      <td>-0.043570</td>\n",
       "      <td>-0.048404</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>0.078919</td>\n",
       "      <td>0.047864</td>\n",
       "      <td>-0.030009</td>\n",
       "      <td>-0.100760</td>\n",
       "      <td>-0.134475</td>\n",
       "      <td>-0.019995</td>\n",
       "      <td>0.009301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.068743</td>\n",
       "      <td>0.222671</td>\n",
       "      <td>0.326633</td>\n",
       "      <td>0.078398</td>\n",
       "      <td>0.248642</td>\n",
       "      <td>0.106344</td>\n",
       "      <td>0.181388</td>\n",
       "      <td>0.117572</td>\n",
       "      <td>0.227606</td>\n",
       "      <td>0.300547</td>\n",
       "      <td>0.123556</td>\n",
       "      <td>0.147437</td>\n",
       "      <td>0.244079</td>\n",
       "      <td>0.297928</td>\n",
       "      <td>0.237182</td>\n",
       "      <td>0.140909</td>\n",
       "      <td>0.126187</td>\n",
       "      <td>0.066917</td>\n",
       "      <td>0.159802</td>\n",
       "      <td>0.211709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     -0.116442     0.015345     0.125402    -0.098801     0.020642   \n",
       "std       0.288796     0.297414     0.288240     0.278640     0.335903   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.303993    -0.191731    -0.055399    -0.286264    -0.205822   \n",
       "50%      -0.102595     0.030105     0.126738    -0.108769     0.020882   \n",
       "75%       0.068743     0.222671     0.326633     0.078398     0.248642   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     -0.103587    -0.012094    -0.079076     0.057976    -0.004344   \n",
       "std       0.318658     0.269757     0.307727     0.270874     0.399235   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.309697    -0.201008    -0.275632    -0.114466    -0.302975   \n",
       "50%      -0.116210    -0.030115    -0.077754     0.054520    -0.030569   \n",
       "75%       0.106344     0.181388     0.117572     0.227606     0.300547   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                10           11           12           13           14  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     -0.052137    -0.061970     0.028560     0.077146     0.051469   \n",
       "std       0.268205     0.313365     0.312077     0.321633     0.302334   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.217847    -0.278649    -0.169166    -0.139485    -0.147503   \n",
       "50%      -0.043570    -0.048404     0.027039     0.078919     0.047864   \n",
       "75%       0.123556     0.147437     0.244079     0.297928     0.237182   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                15           16           17           18           19  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.029513    -0.089124    -0.133571    -0.024268     0.013084  \n",
       "std       0.273468     0.319073     0.301684     0.285178     0.281340  \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000  \n",
       "25%      -0.195425    -0.297408    -0.331173    -0.214304    -0.179786  \n",
       "50%      -0.030009    -0.100760    -0.134475    -0.019995     0.009301  \n",
       "75%       0.140909     0.126187     0.066917     0.159802     0.211709  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_redundant=0, n_informative=2, n_clusters_per_class=1)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X = scaler.fit_transform(X)\n",
    "pd.DataFrame(X).describe()\n",
    "#pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3 балла)** Реализуйте градиентный спуск и протестируйте его для случая логистической регрессии на ранее сгенерированной выборке. Для сравнения качества разных подходов используйте значение оптимизируемого функционала.\n",
    "\n",
    "В качестве критерия останова мы предлагаем использовать следующие условия:\n",
    " - евклидова норма разности текущего и нового векторов весов стала меньше, чем 1e-4\n",
    " - ограничение на число итераций (например, 10000)\n",
    " \n",
    "Для начальной инициализации весов нужно сравнить следующие подходы:\n",
    " - нулевая начальная инициализация\n",
    " - случайная\n",
    " \n",
    "Выполните следующие пункты и прокомментируйте полученные результаты:\n",
    "- Рассмотрите как влияет размер шага на сходимость (попробуйте не менее 5-ти различных значений).\n",
    "- Рассмотрите регуляризованную модель (не менее 5-ти различных коэффициентов регуляризации), которая описана выше, а также модель без регуляризатора. Сравните влияет ли наличие регуляризации на скорость сходимости и качество (под качеством во всех случаях подразумевается значение исходного, нерегуляризованного функционала).\n",
    "- Исследуйте качество оптимизируемого функционала в зависимости от номера итерации (при правильной реализации и подходящем размере шага он должен убывать).\n",
    "- Влияет ли выбор начальной инициализации весов на скорость и качество?\n",
    "\n",
    "В каждом пункте требуется построить необходимые графики скорости/качества и дать исчерпывающие выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градиентного спуска может быть весьма трудозатратен в случае большого размера обучающей выборки. Поэтому обычно используют метод стохастического градиентного спуска, где на каждой итерации выбирается случайный объект из обучающей выборки и обновление весов происходит сразу по этому объекту. \n",
    "\n",
    "**(1.5 балл)** Реализуйте метод стохастического градиентного спуска (sgd). В этом случае вы можете выбрать наиболее удачный функционал, исходя из предыдущего пункта (с регуляризацией, без), а также схему начальной инициализации весов. Реализуйте метод в виде класса, удовлетворяющего интерфейсу scikit-learn ([тут есть пример](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/)).\n",
    "\n",
    "Сравните рассмотренные методы (градиентный спуск и sgd) между собой с точки зрения скорости сходимости и качества.\n",
    "\n",
    "- Посмотрите как влияет размер шага на сходимость (попробуйте 4-5 различных значений)\n",
    "- Исследуйте качество оптимизируемого функционала в зависимости от номера итерации\n",
    "\n",
    "Выберите лучший размер шага и сравните качество и скорость сходимости реализованного метода и [его аналога из scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) (не забудьте выставить значения важных параметров tol и learning_rate для сравнения методов в приближённо равных условиях). Насколько получилось лучше/хуже, в чём могут быть причины?\n",
    "\n",
    "В каждом пункте сделайте исчерпывающие выводы, подкреплённые графиками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Добавьте в выборку случайный шум следующим способом:\n",
    " 1. Выберите случайно 3% объектов из сгенерированной ранее выборки.\n",
    " 2. Сдвиньте эти объекты на $\\mathcal{N}(0; 3 * scale)$, где $scale$ —  масштаб, который был использован при генерации объектов (параметр $scale$ в make_classification).\n",
    " 3. Инвертируйте классы выбранных объектов.\n",
    " \n",
    "Для GD и SGD сравните скорость сходимости и значение функции потерь до и после добавления шума. Сделайте необходимые выводы, подкреплённые графиками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Между обновлением вектора весов по всей выборке и на одном объекте есть промежуточный подход — выбирать некоторое случайное подмножество объектов и обновлять веса по нему. Такой подход называется mini-batch. Мы не будем реализовывать этот подход в данной работе, однако иногда его бывает осмысленно использовать на практике. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из недостатков sgd состоит в том, что он может не доходить до локального оптимального решения, а осциллировать в окрестности. \n",
    "\n",
    "![](http://sebastianruder.com/content/images/2015/12/without_momentum.gif)\n",
    "\n",
    "Для решения этой проблемы существуют методы, позволяющие устранить этот недостаток, а также ускорить сходимость. Рассмотрим некоторые из них.\n",
    "\n",
    "![](http://nghenglim.github.io/images/2015061300.png)\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ v_t = \\gamma v_{t - 1} + \\eta \\nabla_{\\theta}{J(\\theta)}$$\n",
    "$$ \\theta = \\theta - v_t$$\n",
    "\n",
    "где\n",
    "\n",
    " - $\\theta$ — вектор параметров (в нашем случае — $w$)\n",
    " - $J$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n",
    " \n",
    "### Adagrad \n",
    "\n",
    "Одной из сложностей является выбор размера шага (*learning rate*). Основное отличие данного метода от SGD состоит в том что размер шага определяется для каждого параметра индивидуально. Этот метод хорошо работает с разреженным данным большого объема. \n",
    "\n",
    "Обозначим градиент по параметру $\\theta_i$ на итерации $t$ как $g_{t,i} = \\nabla_{\\theta}J(\\theta_i)$. \n",
    "\n",
    "В случае sgd обновление параметра $\\theta_i$ будет выглядеть следующим образом:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\eta \\cdot g_{t,i}$$\n",
    "\n",
    "А в случае Adagrad общий шаг $\\eta$ нормируется на посчитанные ранее градиенты для данного параметра:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t,ii} + \\varepsilon}} \\cdot g_{t,i}$$\n",
    "\n",
    "где $G_t$ — диагональная матрица, где каждый диагональный элемент $i,i$ — сумма квадратов градиентов для $\\theta_{i}$ до $t$-ой итерации. $\\varepsilon$ — гиперпараметр, позволяющий избежать деления на 0 (обычно выбирается около *1e-8*).\n",
    "\n",
    "Так как матрица $G_t$ диагональна, в векторном виде это будет выглядеть следующим образом (здесь $\\odot$ — матричное умножение):\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\odot g_t $$\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "Adadelta, в отличии от Adagrad, рассматривает не все предыдущие значения градиентов, а только последние $k$. Кроме того, сумма градиентов определяется как уменьшающееся среднеее всех предыдущих квадратов градиентов. Текущее среднее $E[g^2]_t$ на итерации $t$ будет вглядеть следующим образом:\n",
    "\n",
    "$$ E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g_t^2 $$\n",
    "\n",
    "здесь $\\gamma$ аналогична гиперпараметру из метода Momentum.\n",
    "\n",
    "Тогда обновление весов можно записать следующим образом:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Перепишем это немного по-другому:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\Delta \\theta_t$$ \n",
    "$$\\Delta \\theta_t = - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Аналогично среднему для градиентов определим среднее для параметров $\\theta$:\n",
    "\n",
    "$$ E[\\Delta \\theta^2]_t = \\gamma E[\\Delta \\theta^2]_{t-1} + (1-\\gamma)\\Delta \\theta^2 $$\n",
    "\n",
    "Введем обозначение $RMS[p]_t = \\sqrt{E[p]_t + \\varepsilon}$\n",
    "\n",
    "Тогда Adadelta выглядит следующим образом:\n",
    "\n",
    "$$\\Delta \\theta_t = - \\dfrac{RMS[\\Delta \\theta^2]}{RMS[ga^2]} g_t $$ \n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\Delta \\theta_t$$ \n",
    "\n",
    "\n",
    "Более подробно об этих и других способах оптимизации можно прочитать:\n",
    " - [здесь](http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms) очень хорошее описание различных способов оптимизации, в этом задании мы опираемся на терминологию из данной статьи\n",
    " - статья про [momentum](https://pdfs.semanticscholar.org/97da/c94ffd7a7ac09a4218848300cc7e98569d77.pdf)\n",
    " - оригинальная статья про [adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    " - оригинальная статья про [adadelta](http://arxiv.org/pdf/1212.5701v1.pdf)\n",
    " - википедия про [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) и [adagrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad)\n",
    " - [визуализация](http://imgur.com/a/Hqolp) разных способов оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Реализуйте метод оптимизации *Momentum* **(0.5 балла)** и один из *Adagrad*/*Adadelta* **(1 балл)**.\n",
    "- Сравните оба метода с классическим sgd с точки зрения скорости сходимости.\n",
    "- Посмотрите как значение гиперпараметра $\\gamma$ влияет на скорость сходимости и качество в методе *Momentum*.\n",
    "\n",
    "В заданиях выше требуется построить графики и описать полученные результаты.\n",
    "\n",
    "Дало ли преимущество использование адаптивного шага в методе *Adagrad*/*Adadelta*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многоклассовая классификация\n",
    "\n",
    "Модель логистической регрессии можно обобщить для случая многоклассовой классификации. Метка класса теперь лежит во множестве $\\{1, 2, ..., K\\}$. Параметры модели $w$ в этом случае являются матрицей размерности $K \\times M$, где $M$ − количество признаков. Обучение модели логистической регрессии в многоклассовом случае будет выглядеть следующим образом:\n",
    "\n",
    "$$ -\\dfrac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K [y_i = k] \\log\\Big(\\frac{\\exp(\\langle w_k, x_i \\rangle)}{\\sum_{s=1}^K \\exp(\\langle w_s, x_i \\rangle))}\\Big) + \\dfrac{C}{2}\\lVert w \\rVert^2  \\to \\min_w$$\n",
    "\n",
    "Здесь $w_k$ обозначает $k$-ую строку матрицы $w$.\n",
    "\n",
    "Обучать эту модель также можно с помощью градиентного спуска.\n",
    "\n",
    "Кроме того существует другой, более универсальный способ решать задачу многоклассовой классификации. Для этого нужно обучить несколько бинарных моделей классификации, после чего на основании предсказаний по этим моделям вынести окончательный вердикт о принадлежности объекта одному из $K$ классов. Существует две популярные стратегии использования бинарных классификаторов для задачи многоклассовой классификации:\n",
    " - OvR (One-vs-Rest, One-vs-All) − стратегия, при которой каждый из $K$ классификаторов обучается отделять объекты одного класса от объектов всех остальных классов. В качестве предсказания используется тот класс, классификатор которого предсказал наибольшую вероятность среди всех.\n",
    " - OvO (One-vs-One) − стратегия, при которой каждый из $\\frac{K(K-1)}{2}$ классификаторов учится разделять объекты пары классов, игнорируя объекты всех остальных классов. На этапе предсказания класс обычно выбирается путем голосования по вердиктам каждого из классификаторов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Покажите, что функция потерь для многоклассовой классификации сводится к [функции потерь для бинарной классификации](#log_reg) при $K=2$.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** \n",
    "Сгенерируйте несколько выборок точек с 2 признаками и 3 классами (по 100 объектов каждого класса) на которых будете проводить эксперименты. Для этого можно воспользоваться функцией [make_blobs](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) из пакета sklearn.\n",
    "\n",
    "Обучите [логистическую регрессию](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) из пакета sklearn тремя различными способами:\n",
    " - в режиме multinomial, оптимизирующем многоклассовую функцию потерь\n",
    " - в режиме OvR\n",
    " - в режиме OvO\n",
    " \n",
    "Первые два способа реализованы в самом классе LogisticRegression, в то время как для решения задачи третьим методом в sklearn реализован класс [OneVsOneClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html) (класс для OvR схемы [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html), конечно, также присутствует в пакете).\n",
    " \n",
    " \n",
    " Проделайте следующие шаги для каждой стратегии и прокомментируйте полученные результаты:\n",
    " - Изобразите точки выборки, а также разделяющие прямые (их должно быть по 3 для каждой из стратегий). Проведите эксперимент на всех сгенерированных выборках.\n",
    " - Какие особенности, преимущества и недостатки с точки зрения построения разделяющих плоскостей, качества разделения классов и вычислительной эффективности характерны для каждого метода? Дайте развёрнутый ответ.\n",
    " - Для каждой из стратегий приведите примеры ситуаций, когда стоит выбирать ее для решения задачи многоклассовой классификации. Обоснуйте\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия\n",
    "\n",
    "![](http://66.147.244.197/~globerov/introspectivemode/wp-content/uploads/2012/08/regression-265x300.jpeg)\n",
    "\n",
    "Метод градиентного спуска позволяет оптимизировать произвольные функции. Например, рассмотрим задачу линейной регрессии, где $y \\in \\mathbb{R}$, а алгоритм будет иметь вид $a(x) = \\langle w, x\\rangle$. В случае метода наименьших квадратов оптимизируемый функционал можно записать следующим образом:\n",
    "\n",
    "$$ \\sum_{i=1}^N (\\langle w, x_i \\rangle - y_i) ^ 2 \\to \\min_w$$\n",
    "\n",
    "Эта задача интересна тем, что для нее можно выписать аналитическое решение. Попробуем сравнить эти подходы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте выборку из 600 точек с двумя признаками для задачи регрессии, воспользовавшись функцией [make_regression](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Выпишите как выглядит точное решение задачи линейной регрессии. Решите задачу регрессии с помощью этого подхода без использования и с использованием регуляризации. Есть ли недостатки у такого подхода к решению задачи?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Примените метод стохастического градиентного спуска реализованный ранее. Сильно ли отличается полученный вектор параметров по сравнению с точным решением? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, линейная регрессия позволяет хорошо восстанавливать *линейные* зависимости, однако в общем случае хуже работает с более сложными данными. Это хорошо можно увидеть на следующем примере.\n",
    "\n",
    "Пусть исходная зависимость имеет вид $y = x \\cdot sin(x)$. Сгенерируем соответствующие точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 10, 100)\n",
    "y = X * np.sin(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plt.scatter(X_train, y_train)\n",
    "_ = plt.scatter(X_test, y_test, c='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если теперь к полученным данным применить модель линейной регрессии, то получим следующее решение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train[:, np.newaxis], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plt.scatter(X_train, y_train)\n",
    "_ = plt.scatter(X_test, y_test, c='g')\n",
    "y_plot = lr.predict(X[:, np.newaxis])\n",
    "_ = plt.plot(X, y_plot, c='r')\n",
    "_ = plt.legend(('linear regression', 'train', 'test'), bbox_to_anchor=(1.05, 1), loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество модели регрессии можно оценивать с помощью некоторых метрик, например $MSE = \\sum_{i=1}^l(a(x_i) - y_i)^2$ и в данном случае оно равно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, lr.predict(X_test[:, np.newaxis])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построенное решение совсем отдаленно напоминает исходную зависимость. Одним из способов улучшить результат является добавление всех попарных произведений признаков, а также степеней: $x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "**(0.5 балла)** Воспользуйтесь классом [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) и добавьте к данным зависимости $y = x \\cdot sin(x)$ различные полиномы.\n",
    "\n",
    " - рассмотрите как степень полинома (от 1 до 20) влияет на качество\n",
    " - изобразите на графике предсказание аналогично линейной регрессии\n",
    " - сравните этот подход с функцией [polyfit](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.polyfit.html)\n",
    "\n",
    "В чем могут быть недостатки такого подхода?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный SVM\n",
    "\n",
    "Вернемся к задаче бинарной классификации. Будем обозначать обучающую выборку $\\{(x_n, y_n)\\}_{n=1}^N$, где $N$ — количество объектов, $\\boldsymbol x_n \\in \\mathbb{R}^d$ — числовой вектор признакового описания объекта, $y_n \\in \\{+1, -1\\}$ — класс объекта.\n",
    "\n",
    "SVM обучает модель разделяющей гиперплоскости:\n",
    "$$f(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + b$$\n",
    "Параметры модели — вектор весов $\\boldsymbol w \\in \\mathbb{R}^d$ и сдвиг $b \\in \\mathbb{R}$.\n",
    "\n",
    "Обучение модели происходит путем решения оптимизационной задачи:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\frac{1}{2} \\| \\boldsymbol w \\|^2 + C \\sum_{n=1}^N \\xi_n \\to \\min_{\\boldsymbol w, \\boldsymbol \\xi, b} \\\\\n",
    "    \\text{s.t.: } \\quad y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1 - \\xi_n, \\quad \\xi_n \\geq 0, \\quad \\forall n=1,\\dots,N\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Ограничения вида $y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1$ требуют, чтобы объекты правильно классифицировались разделяющей гиперплоскостью. Поскольку линейная разделимость выборки не гарантируется на практике, вводят переменные $\\xi_n$ (slack variables), которые ослабляют ограничения правильной классификации. В оптимизируемом функционале слагаемое $\\| \\boldsymbol w \\|^2$ штрафует малую ширину разделяющей гиперплоскости, сумма $\\sum_n \\xi_n$ штрафует ослабление ограничений. \n",
    "\n",
    "После нахождения решения оптимизационной задачи $(\\boldsymbol w_{\\star}, \\boldsymbol \\xi_{\\star}, b_{\\star})$, часть ограничений становятся _активными_, т.е. переходят в \"крайнее положение\" — точное равенство:\n",
    "$$\\quad y_n (\\boldsymbol w_{\\star}^T \\boldsymbol x_n + b_{\\star}) = 1 - \\xi_{\\star,n}$$\n",
    "Объекты, соответствующие активным ограничениям, называются _опорными_.\n",
    "\n",
    "Гиперпараметр $C$ задает баланс между шириной разделяющей полосы и ошибками, допускаемыми классификатором. Обратите внимание, что $C$ фиксируется до обучения и не оптимизируется вместе с параметрами модели. Этот гиперпараметр отвечает за обобщающую способность разделяющей гиперплоскости, высокая обобщающая способность (соответствующая большому значению $C$) может привести к переобучению, если линейная модель хорошо описывает обучающие примеры. При подборе оптимального гиперпараметра $C$ необходимо оценивать качество на отложенной выборке или кросс-валидации. Как правило, для конкретной задачи заранее неизвестно, какой порядок имеет оптимальное значение гиперпараметра $C$, поэтому перебирать значения лучше по логарифмической сетке, например: $10^{-3}, 10^{-2}, \\dots, 10^{5}$.\n",
    "\n",
    "Особенность этого метода в том, что он имеет решение, которое может быть найдено используя квадратичное программирование. В этом задании мы не будем сводить данную задачу к задаче квадратичного программирования, а воспользуемся готовой реализацией из библиотеки sklearn.\n",
    "\n",
    "### Особенности реализации\n",
    "\n",
    "Обратите внимание, что в библиотеке sklearn можно найти 2 реализации линейного SVM: [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) и [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) с ядровой функцией *linear*. Эти реализации имеют различие в библиотеках, на которых основаны: в первом случае используется библиотека *liblinear*, во втором — *libsvm*. Каждая из библиотек имеет свои плюсы, поэтому перед применением стоит определиться какая из реализаций подходит больше. Обратите внимание, что это различие есть только для линейного SVM.\n",
    "\n",
    "В данном задании рекомендуем использовать класс [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) с параметром *kernel='linear'*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте три случайные двумерные выборки для бинарной классификации (хотя бы по 400 точек в каждой):\n",
    "- с линейно-разделимыми классами\n",
    "- с хорошо разделимыми классами, но не линейно\n",
    "- с плохо разделимыми классами по имеющимся признакам\n",
    "    \n",
    "Для генерации случайной выборки можно воспользоваться функциями, которые находятся в пакете [sklearn.datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets). Для того чтобы выборки не менялись при перезапуске ноутбука, фиксируйте параметр *random_state*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** Протестируйте линейный SVM  на сгенерированных выборках. Покажите на плоскости разделяющую прямую и линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Проделайте это для разных значений параметра $C$ (не менее 4-х)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** Как зависит число опорных векторов от параметра $C$ для различных выборок?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Явное преобразование признаков\n",
    "\n",
    "Как и в случае с линейной регрессией, когда оптимальная разделяющая гиперплоскость не является линейной, данная модель является очень грубым решением. Линейная неразделимость объектов может быть исправлена путем перехода в другое признаковое пространство, в котором линейная модель лучше описывает данные и, возможно, существует правильно классифицирующая разделяющая гиперплоскость:\n",
    "\n",
    "$$\\boldsymbol x \\in \\mathbb{R}^d \\mapsto \\phi(\\boldsymbol x) \\in \\mathbb{R}^t$$\n",
    "\n",
    "Так, например, аналогичное добавление всех попарных произведений признаков: $\\phi(x_1, \\dots, x_d) = (x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2)$ переводит в пространство, в котором линейная гиперплоскость является квадратичной формой в исходном пространстве и в исходном пространстве признаков разделяющая поверхность может быть, скажем, эллипсом.\n",
    "\n",
    "[Видеоролик с демонстрацией](https://youtu.be/9NrALgHFwTo)\n",
    "\n",
    "### Задание\n",
    "\n",
    "**(1 балл)** На тех же данных используя явное преобразование признаков обучите методом опорных векторов квадратичную разделяющую поверхность. Покажите на плоскости разделяющую прямую и линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Проделайте это для разных значений параметра $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двойственный переход и Ядровой SVM\n",
    "\n",
    "![](http://i.imgur.com/bJAzRCt.png)\n",
    "\n",
    "Задачу обучения линейного SVM, рассмотренную в предыдущем пункте принято называть _прямой_ оптимизационной задачей для SVM. Любая задача оптимизации с ограничениями имеет [_двойственную_ задачу Лагранжа](http://goo.gl/OujTPr), в которой оптимизируются _двойственные переменные_ (множители Лагранжа), соответствующие штрафу за нарушение ограничений, максимизируется нижняя оценка функционала прямой задачи. В случае задачи квадратичного программирования, решение двойственной задачи (значение оптимизируемого функционала) совпадает с оптимумом прямой задачи.\n",
    "\n",
    "Подробнее можно почитать в [статье](http://www.machinelearning.ru/wiki/images/2/25/SMAIS11_SVM.pdf).\n",
    "\n",
    "Двойственная задача для SVM имеет вид:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\sum_{n} \\alpha_n - \\frac{1}{2}\\sum_{n}\\sum_{n'} \\alpha_{n}\\alpha_{n'} y_{n}y_{n'} x_{n}^Tx_{n'} \\to \\max_{\\alpha} \\\\\n",
    "    \\begin{aligned}\n",
    "        \\text{s.t. } \\quad  \n",
    "        & 0 \\le \\alpha_n \\le C, \\quad \\forall n = 1, \\dots, N \\\\\n",
    "        & \\sum_{n} \\alpha_n y_n = 0\n",
    "    \\end{aligned}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Оптимизируется вектор из двойственных переменных $\\alpha_n$, соответствующих объектам обучающей выборки. Объект $x_n$ является опорным, если $\\alpha_n > 0$.\n",
    "\n",
    "Предсказание вычисляется по следующему правилу:\n",
    "$$\\hat{y}(x) = \\text{sign}\\left(\\sum_{n}\\alpha_{n}y_{n}x^Tx_{n} + b\\right).$$\n",
    "\n",
    "Для предсказания необходимо оценить значение $b$. Известно, что для любого опорного объекта, который классифицируется безошибочно верно:\n",
    "$$y_n = \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'} + b,$$\n",
    "значит для любого такого объекта:\n",
    "$$b = y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}.$$\n",
    "\n",
    "В случае наличия ошибок классификации обучающей выборки, предлагается усреднять значение $b$ по всем опорным векторам:\n",
    "$$b = \\frac{1}{N_\\text{SV}}\\sum_{n \\in \\text{SV}}\\left(y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}\\right).$$\n",
    "Интуиция здесь такова, что суммарные ошибки в положительную сторону примерно равны суммарным ошибкам в отрицательную сторону.\n",
    "\n",
    "Другой вариант — отказаться от параметра $b$ и работать с моделью $f(x) = w^Tx$, добавив к вектору $x$ константный признак.\n",
    "\n",
    "#### Неявное преобразование признаков\n",
    "Отметим, что двойственная задача SVM содержит вектора признаков исключительно в виде скалярного произведения $x^Tx'$. Эта особенность позволяет производить неявное преобразование признакового пространства. Вместо вычисления функции $\\phi(\\boldsymbol x)$, которая может отображать исходные признаки в вектора очень большой размерности, будем вычислять скалярное произведение $k(\\boldsymbol x, \\boldsymbol x') = \\phi(\\boldsymbol x)^T\\phi(\\boldsymbol x')$ называемое _ядром_. \n",
    "\n",
    "\n",
    "В этом задании используйте класс $sklearn.svm.SVC$, меняя тип ядра. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "**(1.5 балла)** Протестируйте на предыдущих двумерных выборках ядровой SVM. Покажите на плоскости строящиеся разделяющие поверхности, линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Попробуйте следующие ядровые функции:\n",
    "- линейная: $k(x, x') = x^Tx'$\n",
    "- полиномиальная: $k(x, x') = (x^Tx' + 1)^d$ с различными степенями $d = 2,3,\\dots$\n",
    "- Гауссовская-RBF: $k(x, x') = \\exp(-\\sigma\\|x - x'\\|^2)$\n",
    "\n",
    "Ответьте на следующие вопросы:\n",
    " - Как ведет себя SVM с полиномиальным ядром в зависимости от параметров $C$ и степени ядра $d$?\n",
    " - Как ведет себя SVM с RBF-ядром в зависимости от параметров $C$ и $\\sigma$? Поварьируйте параметры $C$ и $\\sigma$ по логарифмической сетке. Какие значения параметров ведут к переобучению, а какие — к слишком грубой модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "**(1 балл)** В этой работе вы рассмотрели некоторые линейные модели машинного обучения, а также способы их обучения. Ответьте на следующие вопросы:\n",
    "\n",
    " - Какие есть достоинства у рассмотренных моделей? Поясните свой ответ для каждой модели.\n",
    " - Каким общим недостатком обладают данные модели? Какие есть способы его устранения? В чем может заключаться сложность использования этого подхода?\n",
    " - В чем заключаются различия с точки зрения обучения алгоритмов? Какие есть достоинства и недостатки у рассмотренных методов обучения?\n",
    " - Предположите в каком случае каждый из алгоритмов будет работать лучше: при большом/небольшом количестве данных? Поясните почему. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
